---
title: "titanic-exploratory-modeling"
author: "shobha mourya"
date: "March 10, 2019"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exploratory Modeling After Data Analysis

From our analysis and visualizations we know the below features have good predictive power:

1. pclass 
2. title 
3. sibsp 
4. parch 
5. family.size


# Exploraoty Modeling using Ensemble method: Random Forest

Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.
Random decision forests correct for decision trees' habit of overfitting to their training set.

What's the difference between bagging and boosting?
Bagging and Boosting are both ensemble methods in Machine Learning, but what's the key behind them? Bagging and Boosting are similar in that they are both ensemble techniques, where a set of weak learners are combined to create a strong learner that obtains better performance than a single one.


Is Random Forest bagging or boosting?

Random forest is a bagging technique and not a boosting technique. In boosting as the name suggests, one is learning from other which in turn boosts the learning. The trees in random forests are run in parallel. There is no interaction between these trees while building the trees.

Build RandomForest models with different features to find out which have the 
most predictive power.


# Testing Random Forest Model with different features

Now let's use random forest model and see which amongst these is more important

1. Train a Random Forest with the default parameters using pclass & title
2. Train a Random Forest using pclass, title, & sibsp
3. Train a Random Forest using pclass, title, & parch
4. Train a Random Forest using pclass, title, sibsp, parch
5. Train a Random Forest using pclass, title, & family.size
6. Train a Random Forest using pclass, title, sibsp, & family.size
7. Train a Random Forest using pclass, title, parch, & family.size


```{r}
# Load the libraries
library(dplyr)
library(stringr)
library(ggplot2)

# Load the cleaned dataset 
path = "C:/Users/shobha/Documents/RExploratyDataAnalysis"
data.combined = readRDS(paste0(path,"/RdsObjects/data.combined.rds"))

glimpse(data.combined)

# Extract the training data
train = data.combined[1:891, ]
names(train)
```

## Set up the label as a factor

```{r}
library(randomForest)

# Set the label as factor for binary classifcation using RandomForest
rf.label <- as.factor(train$survived)

str(rf.label)

levels(rf.label)

table(rf.label)

# drop the unused levels
rf.label = droplevels(rf.label)

table(rf.label)

head(rf.label)
```

## 1. RandomForest Model with features pclass,title

```{r eval=FALSE}
# Train a Random Forest with the default parameters using pclass & title

rf.train.1 <- data.combined[1:891, c("pclass", "title")]

set.seed(1234)

rf.1 <- randomForest(x = rf.train.1, y = rf.label, importance = TRUE, ntree = 1000)
rf.1

saveRDS(rf.1, paste0(path,"/RdsObjects/rf.1.rds"))
```

```{r}
rf.1 = readRDS(paste0(path,"/RdsObjects/rf.1.rds"))
rf.1
```

```{r}
rf.1.OOB = 20.31

rf.1.accuracy = (533+177)/(533+16+177+165)
rf.1.accuracy = round(rf.1.accuracy*100, 2)
rf.1.accuracy

varImpPlot(rf.1)

```

### What is OOB

Out-of-bag (OOB) error, also called out-of-bag estimate, is a method of measuring 
the prediction error of random forests, boosted decision trees, and other machine 
learning models utilizing bootstrap aggregating (bagging) to sub-sample data samples 
used for training.


### Analysis rf.1 with predictors title and pclass

OOB error estimate is 20.31 and title is more important than pclass for prediction.
Now, let's include sibsp to see if it reduces the error.


## 2. RandomForest Model with features pclass,title, sibsp

```{r eval=FALSE}
# Train a Random Forest using pclass, title, & sibsp
rf.train.2 <- data.combined[1:891, c("pclass", "title", "sibsp")]

set.seed(1234)
rf.2 <- randomForest(x = rf.train.2, 
                     y = rf.label, 
                     importance = TRUE, 
                     ntree = 1000)
rf.2

#saveRDS(rf.2, paste0(path,"/RdsObjects/rf.2.rds"))
```


```{r}

rf.2 = readRDS(paste0(path,"/RdsObjects/rf.2.rds"))
rf.2
```

```{r}
rf.2.OOB = 18.63

rf.2.accuracy = (489+236)/(489+60+236+106)
rf.2.accuracy = round(rf.2.accuracy*100, 2)
rf.2.accuracy

varImpPlot(rf.2)
```

### Analysis rf.2 with predictors title, pclass and sibsp

OOB error estimate has reduced from 20.31 to 18.63 by including sibsp along with
title and pclass

Features by importance are title, pclass, sibsp. Now lets replace sibsp with parch.

## 3. RandomForest Model with features pclass, title, parch

```{r eval=FALSE}
# Train a Random Forest using pclass, title, & parch
rf.train.3 <- data.combined[1:891, c("pclass", "title", "parch")]

set.seed(1234)
rf.3 <- randomForest(x = rf.train.3, y = rf.label, 
                     importance = TRUE, 
                     ntree = 1000)
rf.3

saveRDS(rf.3, paste0(path,"/RdsObjects/rf.3.rds"))
```


```{r}

rf.3 = readRDS(paste0(path,"/RdsObjects/rf.3.rds"))
rf.3
```


```{r}
rf.3.OOB = 19.3

rf.3.accuracy = (492+227)/(489+57+115+227)
rf.3.accuracy = round(rf.3.accuracy*100, 2)
rf.3.accuracy

varImpPlot(rf.3)
```

### Analysis rf.3 with predictors title, pclass and sibsp

OOB error estimate has increase from 18.63 to 19.3, so title, pclass and sibsp 
have better predictive power than title,pclass, parch.

Features by importance title, pclass, parch.

Let's test will all four features title, pclass, sibsp, parch

## 4. RandomForest Model with features pclass, title, sibsp, parch

```{r eval=FALSE}
# Train a Random Forest using pclass, title, sibsp, parch
rf.train.4 <- data.combined[1:891, c("pclass", "title", "sibsp", "parch")]

set.seed(1234)
rf.4 <- randomForest(x = rf.train.4, y = rf.label, importance = TRUE, ntree = 1000)
rf.4

saveRDS(rf.4, paste0(path,"/RdsObjects/rf.4.rds"))
```

```{r}
rf.4 = readRDS(paste0(path,"/RdsObjects/rf.4.rds"))
rf.4
```


```{r}
rf.4.OOB = 18.41

rf.4.accuracy = (490+237)/(490+59+105+237)
rf.4.accuracy = round(rf.4.accuracy*100, 2)
rf.4.accuracy

varImpPlot(rf.4)
```

### Analysis rf.4 with predictors title, pclass, sibsp and parch

OOB error estimate is lowest so far with all the four features 18.41 compared 
to 18.63 with title pclass sibsp.

Now let's try with another feature family.size along with title and pclass.
family size is derived from sibsp and parch, so let's see if it has better 
predictive power than them.

As expected features by importance ar title, pclass, sibsp, parch

## 5. RandomForest Model with pclass, title and derived feature family.size

```{r eval=FALSE}
# Train a Random Forest using pclass, title, & family.size
rf.train.5 <- data.combined[1:891, c("pclass", "title", "family.size")]

set.seed(1234)
rf.5 <- randomForest(x = rf.train.5, y = rf.label, importance = TRUE, ntree = 1000)
rf.5

saveRDS(rf.5, paste0(path,"/RdsObjects/rf.5.rds"))
```

```{r}
rf.5 = readRDS(paste0(path,"/RdsObjects/rf.5.rds"))
rf.5
```

```{r}
rf.5.OOB = 18.97

rf.5.accuracy = (485+237)/(485+64+105+237)
rf.5.accuracy = round(rf.5.accuracy*100, 2)
rf.5.accuracy

varImpPlot(rf.5)
```

### Analysis rf.5 with predictors title, pclass and family.size


OOB error estimate has increased a bit with family.size than with sibsp, and importance
is still title, pclass. Let's try with sibsp and family.size included.

## 6.RandomForest Model with pclass, title, sibsp and derived feature family.size

```{r eval=FALSE}
# Train a Random Forest using pclass, title, sibsp, & family.size
rf.train.6 <- data.combined[1:891, c("pclass", "title", "sibsp", "family.size")]

set.seed(1234)
rf.6 <- randomForest(x = rf.train.6, y = rf.label, importance = TRUE, ntree = 1000)
rf.6

saveRDS(rf.6, paste0(path,"/RdsObjects/rf.6.rds"))
```

```{r}
rf.6 = readRDS(paste0(path,"/RdsObjects/rf.6.rds"))
rf.6
```


```{r}
rf.6.OOB = 18.41

rf.6.accuracy = (487+240)/(487+62+102+240)
rf.6.accuracy = round(rf.6.accuracy*100, 2)
rf.6.accuracy

varImpPlot(rf.6)
```

### Analysis rf.6 with predictors title, pclass, sibsp and family.size

OOB error estimate is less by including sibsp and family.size but family.size is more 
important than sibsp; title and pclass being most important.
Now, let's include parch instead of sibsp

## 7.RandomForest Model with pclass, title, parch and derived feature family.size

```{r eval=FALSE}
# Train a Random Forest using pclass, title, parch, & family.size
rf.train.7 <- data.combined[1:891, c("pclass", "title", "parch", "family.size")]

set.seed(1234)
rf.7 <- randomForest(x = rf.train.7, y = rf.label, importance = TRUE, ntree = 1000)
rf.7

saveRDS(rf.7, paste0(path,"/RdsObjects/rf.7.rds"))
```


```{r}
rf.7 = readRDS(paste0(path,"/RdsObjects/rf.7.rds"))
rf.7
```

```{r}
rf.7.OOB = 19.08

rf.7.accuracy = (487+234)/(487+62+108+234)
rf.7.accuracy = round(rf.7.accuracy*100, 2)
rf.7.accuracy

varImpPlot(rf.7)
```

### Analysis rf.7 with predictors title, pclass, parch and family.size

OOB error estimate has increased with parch instead of sibsp and title, pclass, 
family.size still most important features. 
So, we'll use these three features for our final model - title, pclass, family.size

```{r}
accuracy = c(rf.1.accuracy, rf.2.accuracy, rf.3.accuracy, rf.4.accuracy,
                  rf.5.accuracy, rf.6.accuracy, rf.7.accuracy)
names(accuracy) = c("rf.1.accu", "rf.2.accu", "rf.3.accu", "rf.4.accu",
                    "rf.5.accu", "rf.6.accu", "rf.7.accu")

OOB = c(rf.1.OOB, rf.2.OOB, rf.3.OOB, rf.4.OOB, rf.5.OOB, rf.6.OOB, rf.7.OOB)

names(OOB) = c("rf.1.OOB", "rf.2.OOB", "rf.3.OOB", "rf.4.OOB", "rf.5.OOB", 
               "rf.6.OOB", "rf.7.OOB")



accuracy
str(accuracy)
class(accuracy)

sort(accuracy, decreasing = TRUE)
sort(OOB, decreasing = FALSE)

```

## Which model to pick?

In order of highest accuracy rf.4 and rf.6 have same accuracy

rf.4 - title,plcass,sibsp,parch
rf.6 - title, plcass,sibsp,family.size
rf.2 - title. pclass, sibsp
rf.5 - title.pclass,family.size

In order to avoid overfitting and also since family.size is derived from sibsp 
and parch, rf.5 is the best model for prediction.


## Submission of rf.5

Before we jump into features engineering we need to establish a methodology
for estimating our error rate on the test set (i.e., unseen data). This is
critical, for without this we are more likely to overfit. Let's start with a 
submission of rf.5 to Kaggle to see if our OOB error estimate is accurate.

```{r}
# rf.5 is trained with pclass, title and family.size
rf.train.5 <- data.combined[1:891, c("pclass", "title", "family.size")]

# Subset our test dataset to include features used in training the model
test.submit.df <- data.combined[892:1309, c("pclass", "title", "family.size")]


# Now predict the label survived using the model 

rf.5.preds = predict(rf.5, test.submit.df)
str(rf.5.preds)
class(rf.5.preds)
table(rf.5.preds)
```

The result is a vector of factor variable

Write out a CSV file for submission to Kaggle

```{r}
# Store the results of prediction for label Survived in data frame
submit.df <- data.frame(PassengerId = rep(892:1309), Survived = rf.5.preds)

#write.csv(submit.df, file = "RF_SUB_20160215_1.csv", row.names = FALSE)
write.csv(submit.df, file = "RF_SUB_20190225_1.csv", row.names = FALSE)
```

Submission score

Our submission scores 0.79426, but the OOB predicts that we should score 0.8159.
Let's look into cross-validation using the caret package to see if we can get
more accurate estimates.

# Improve the Performace using caret stratified testing

Now that we have decided upon the ensemble model, let's look at ways to improve the accuracy of the model using cross validation.



## Stratified Cross Validation

Cross Validate using caret package


```{r}
#install.packages("caret")
library(caret)
```

Research has shown that 10-fold CV repeated 10 times is the best place to start,
however there are no hard and fast rules - this is where the experience of the 
Data Scientist (i.e., the 'art') comes into play. We'll start with 10-fold CV,
repeated 10 times and see how it goes.

Leverage caret to create 100 total folds, but ensure that the ratio of those
that survived and perished in each fold matches the overall training set. This
is known as stratified cross validation and generally provides better results.

```{r}
# Proportion of label
table(rf.label)
```

```{r}
(342/549)*100
```

62 percent perished

## 1. Create stratified folds 

```{r}
#Create stratified folds and check if the survival ration is same as in the original

set.seed(2348)

cv.10.folds <- createMultiFolds(rf.label, k = 10, times = 10)

class(cv.10.folds)

length(cv.10.folds)
```

It is a list of 100 items, 10 folds repeated 10 times generates list of 100 items.
Each item is an array or vector of 100 numbers indicating the index of the dataset
to be used to train the model.
We will use randomForest on the stratified training dataset.


```{r}
# What is the 33 rd item in the list
str(cv.10.folds[[33]])

```

Integer Vector


```{r}
head(cv.10.folds[[33]], 40)
```

It is arrary of numbers to be used to index test dataset for training the model with 10 fold cv.
It may look like all numbers are included but if you look closely few indexs are missing like 28.

```{r}
# Check stratification for 33rd fold
table(rf.label[cv.10.folds[[33]]])
```

```{r}
(308/494)*100
```

62 percent perished, same as in original test dataset.

## 2. Set up trainControl 

```{r}
# Set up caret's trainControl object for the cv folds we created above.
ctrl.1 <- trainControl(method = "repeatedcv", number = 10, repeats = 10,
                       index = cv.10.folds)

```

Set up doSNOW package for multi-core training. This is helpful as we're going
to be training a lot of trees.
NOTE - This works on Windows and Mac, unlike doMC

## 3. Parallel Execution 

```{r}

#install.packages("doSNOW")
library(doSNOW)
```


### 1. Parallel processing with three sockets cluster and 10-fold CV

```{r eval=FALSE}

#cl <- makeCluster(6, type = "SOCK")
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)


# Set seed for reproducibility and train
set.seed(34324)
rf.5.cv.1 <- train(x = rf.train.5, y = rf.label, method = "rf", tuneLength = 3,
                   ntree = 1000, trControl = ctrl.1)

#Shutdown cluster
stopCluster(cl)

# Check out results
rf.5.cv.1

saveRDS(rf.5.cv.1,  paste0(path,"/RdsObjects/rf.5.cv.1.rds"))
rm(rf.5.cv.1)
rf.5.cv.1

```


```{r}
rf.5.cv.1 = readRDS(paste0(path,"/RdsObjects/rf.5.cv.1.rds"))
rf.5.cv.1
```

```{r}
#With the three features: mtry = 3

rf.5.cv.1.accuracy = 0.8104*100
rf.5.cv.1.accuracy
rf.5.accuracy
```

The above is only slightly more pessimistic than the rf.5 OOB prediction, but 
not pessimistic enough. Let's try 5-fold CV repeated 10 times.

### 2.  Parallel processing with three sockets cluster and 5-fold CV

```{r eval=FALSE}
set.seed(5983)
cv.5.folds <- createMultiFolds(rf.label, k = 5, times = 10)

ctrl.2 <- trainControl(method = "repeatedcv", number = 5, repeats = 10,
                       index = cv.5.folds)

cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)

set.seed(89472)
rf.5.cv.2 <- train(x = rf.train.5, y = rf.label, method = "rf", tuneLength = 3,
                   ntree = 1000, trControl = ctrl.2)

#Shutdown cluster
stopCluster(cl)

# Check out results
rf.5.cv.2

saveRDS(rf.5.cv.2,  paste0(path,"/RdsObjects/rf.5.cv.2.rds"))
rm(rf.5.cv.2)
rf.5.cv.2

```


```{r}
rf.5.cv.2 = readRDS(paste0(path,"/RdsObjects/rf.5.cv.2.rds"))
rf.5.cv.2
```


With three features, i.e mtry=3 :
rf.5 accuracy was higher this one 0.8109
lesser data to train reduces overfitting so let's try 3-fold

```{r}
rf.5.cv.2.accuracy = 0.8107*100
rf.5.cv.2.accuracy
rf.5.cv.1.accuracy
rf.5.accuracy
```


### 3. 5-fold CV isn't better. Move to 3-fold CV repeated 10 times. 

```{r eval=FALSE}
set.seed(37596)
cv.3.folds <- createMultiFolds(rf.label, k = 3, times = 10)

ctrl.3 <- trainControl(method = "repeatedcv", number = 3, repeats = 10,
                       index = cv.3.folds)

cl <- makeCluster(6, type = "SOCK")
registerDoSNOW(cl)

set.seed(94622)
rf.5.cv.3 <- train(x = rf.train.5, y = rf.label, method = "rf", tuneLength = 3,
                   ntree = 64, trControl = ctrl.3)

#Shutdown cluster
stopCluster(cl)

# Check out results
rf.5.cv.3

saveRDS(rf.5.cv.3,  paste0(path,"/RdsObjects/rf.5.cv.3.rds"))
rm(rf.5.cv.3)
rf.5.cv.3

```

```{r}
rf.5.cv.3 = readRDS(paste0(path,"/RdsObjects/rf.5.cv.3.rds"))
rf.5.cv.3
```

With three features, mtry = 3

```{r}
rf.5.cv.3.accuracy = 0.807*100
rf.5.cv.3.accuracy
rf.5.cv.2.accuracy
rf.5.cv.1.accuracy
rf.5.accuracy
```

Accuracy improved to 0.8101 with lesser data

# Exploratory Modeling 2 : Using Decision Tree (instead of RF ensemble modeling)

To see if further feature engineering will help us build better models or not
Decision trees allow us to see the effect of features better than rf
though there is risk of overfitting. 
randomForest builds many decision tress with different combinations of features
hence it counters the effect of overfitting but it is not intuitive
hence to explore the effect of features we'll use decision trees
we see that family.size predicts family.size greater than 4 for pclass 3 all perish this is case of overfitting so we will focus on title.

## Why Single Decision Tree? 

Let's use a single decision tree to better understand what's going on with our
features. Obviously Random Forests are far more powerful than single trees,
but single trees have the advantage of being easier to understand.

```{r}
#install.packages("rpart")
#install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

# Create utility function
rpart.cv <- function(seed, training, labels, ctrl) {
  cl <- makeCluster(3, type = "SOCK")
  registerDoSNOW(cl)
  
  set.seed(seed)
  # Leverage formula interface for training
  rpart.cv <- train(x = training, y = labels, method = "rpart", tuneLength = 30, 
                    trControl = ctrl)
  
  #Shutdown cluster
  stopCluster(cl)
  
  return (rpart.cv)
}
```

## 1. Let's use 3-fold CV repeated 10 times 

```{r eval=FALSE}
# Grab features
features <- c("pclass", "title", "family.size")
rpart.train.1 <- data.combined[1:891, features]

# Run CV and check out results
#rpart.cv <- function(seed, training, labels, ctrl)
rpart.1.cv.1 <- rpart.cv(94622, rpart.train.1, rf.label, ctrl.3)
rpart.1.cv.1

saveRDS(rpart.1.cv.1,  paste0(path,"/RdsObjects/rpart.1.cv.1.rds"))
rm(rpart.1.cv.1)
rpart.1.cv.1
```

```{r}
rpart.1.cv.1 = readRDS(paste0(path,"/RdsObjects/rpart.1.cv.1.rds"))
rpart.1.cv.1
```

### Plot
```{r}
prp(rpart.1.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
```


The plot bring out some interesting lines of investigation. Namely:
1 - Titles of "Mr." and "Other" are predicted to perish at an 
overall accuracy rate of 83.2 %.

(node with 436 81 indicates 436 were correct predictions )
436/(436+81)*100 = 436/517*100 = 83.3 %

2 - Titles of "Master.", "Miss.", & "Mrs." in 1st & 2nd class
are predicted to survive at an overall accuracy rate of 94.9%.

3 - Titles of "Master.", "Miss.", & "Mrs." in 3rd class with 
family sizes equal to 5, 6, 8, & 11 are predicted to perish
with 100% accuracy.

4 - Titles of "Master.", "Miss.", & "Mrs." in 3rd class with 
family sizes not equal to 5, 6, 8, or 11 are predicted to 
survive with 59.6% accuracy.

### Interpretation

Both rpart and rf confirm that title is important, let's investigate further

```{r}
table(data.combined$title)

# Parse out last name and title
data.combined[1:25, "name"]

name.splits <- str_split(data.combined$name, ",")
name.splits[1]
last.names <- sapply(name.splits, "[", 1)
last.names[1:10]

# Add last names to dataframe in case we find it useful later
data.combined$last.name <- last.names

# Now for titles
name.splits <- str_split(sapply(name.splits, "[", 2), " ")
titles <- sapply(name.splits, "[", 2)
unique(titles)

# What's up with a title of 'the'?
data.combined[which(titles == "the"),]

# Re-map titles to be more exact
titles[titles %in% c("Dona.", "the")] <- "Lady."
titles[titles %in% c("Ms.", "Mlle.")] <- "Miss."
titles[titles == "Mme."] <- "Mrs."
titles[titles %in% c("Jonkheer.", "Don.")] <- "Sir."
titles[titles %in% c("Col.", "Capt.", "Major.")] <- "Officer"
table(titles)

# Make title a factor
data.combined$new.title <- as.factor(titles)

# Visualize new version of title
ggplot(data.combined[1:891,], aes(x = new.title, fill = survived)) +
  geom_bar() +
  facet_wrap(~ pclass) + 
  ggtitle("Surival Rates for new.title by pclass")

# Collapse titles based on visual analysis
indexes <- which(data.combined$new.title == "Lady.")
data.combined$new.title[indexes] <- "Mrs."

indexes <- which(data.combined$new.title == "Dr." | 
                   data.combined$new.title == "Rev." |
                   data.combined$new.title == "Sir." |
                   data.combined$new.title == "Officer")
data.combined$new.title[indexes] <- "Mr."

# Visualize 
ggplot(data.combined[1:891,], aes(x = new.title, fill = survived)) +
  geom_bar() +
  facet_wrap(~ pclass) +
  ggtitle("Surival Rates for Collapsed new.title by pclass")
```

```{r eval=FALSE}
# Grab features
features <- c("pclass", "new.title", "family.size")
rpart.train.2 <- data.combined[1:891, features]

# Run CV and check out results
#rpart.cv <- function(seed, training, labels, ctrl)
rpart.2.cv.1 <- rpart.cv(94622, rpart.train.2, rf.label, ctrl.3)
rpart.2.cv.1

saveRDS(rpart.2.cv.1,  paste0(path,"/RdsObjects/rpart.2.cv.1.rds"))
rm(rpart.2.cv.1)
rpart.2.cv.1
```


```{r}
rpart.2.cv.1 = readRDS(paste0(path,"/RdsObjects/rpart.2.cv.1.rds"))
rpart.2.cv.1
```

### Plot

```{r}
prp(rpart.2.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
```


# Further feature engineering of title

```{r}
# Dive in on 1st class Mr."
indexes.first.mr <- which(data.combined$new.title == "Mr." 
                          & data.combined$pclass == "1")
first.mr.df <- data.combined[indexes.first.mr, ]
summary(first.mr.df)

# One female?
first.mr.df[first.mr.df$sex == "female",]

# Update new.title feature
indexes <- which(data.combined$new.title == "Mr." & 
                   data.combined$sex == "female")
data.combined$new.title[indexes] <- "Mrs."

# Any other gender slip ups?
length(which(data.combined$sex == "female" & 
               (data.combined$new.title == "Master." |
                  data.combined$new.title == "Mr.")))

# Refresh data frame
indexes.first.mr <- which(data.combined$new.title == "Mr." & data.combined$pclass == "1")
first.mr.df <- data.combined[indexes.first.mr, ]

# Let's look at surviving 1st class "Mr."
summary(first.mr.df[first.mr.df$survived == "1",])
View(first.mr.df[first.mr.df$survived == "1",])

```

### Feature engineering of fares

```{r}
# Take a look at some of the high fares
indexes <- which(data.combined$ticket == "PC 17755" |
                   data.combined$ticket == "PC 17611" |
                   data.combined$ticket == "113760")
View(data.combined[indexes,])

# Visualize survival rates for 1st class "Mr." by fare
ggplot(first.mr.df, aes(x = fare, fill = survived)) +
  geom_density(alpha = 0.5) +
  ggtitle("1st Class 'Mr.' Survival Rates by fare")


```

### Engineer features based on all the passengers with the same ticket

```{r}
ticket.party.size <- rep(0, nrow(data.combined))
avg.fare <- rep(0.0, nrow(data.combined))
tickets <- unique(data.combined$ticket)

for (i in 1:length(tickets)) {
  current.ticket <- tickets[i]
  party.indexes <- which(data.combined$ticket == current.ticket)
  current.avg.fare <- data.combined[party.indexes[1], "fare"] / length(party.indexes)
  
  for (k in 1:length(party.indexes)) {
    ticket.party.size[party.indexes[k]] <- length(party.indexes)
    avg.fare[party.indexes[k]] <- current.avg.fare
  }
}

data.combined$ticket.party.size <- ticket.party.size
data.combined$avg.fare <- avg.fare

# Refresh 1st class "Mr." dataframe
first.mr.df <- data.combined[indexes.first.mr, ]
summary(first.mr.df)


# Visualize new features
ggplot(first.mr.df[first.mr.df$survived != "None",], aes(x = ticket.party.size, fill = survived)) +
  geom_density(alpha = 0.5) +
  ggtitle("Survival Rates 1st Class 'Mr.' by ticket.party.size")

ggplot(first.mr.df[first.mr.df$survived != "None",], aes(x = avg.fare, fill = survived)) +
  geom_density(alpha = 0.5) +
  ggtitle("Survival Rates 1st Class 'Mr.' by avg.fare")
```

### Hypothesis - ticket.party.size is highly correlated with avg.fare

```{r}
summary(data.combined$avg.fare)

# One missing value, take a look
data.combined[is.na(data.combined$avg.fare), ]

# Get records for similar passengers and summarize avg.fares
indexes <- with(data.combined, which(pclass == "3" & title == "Mr." & family.size == 1 &
                                       ticket != "3701"))
similar.na.passengers <- data.combined[indexes,]
summary(similar.na.passengers$avg.fare)

# Use median since close to mean and a little higher than mean
data.combined[is.na(avg.fare), "avg.fare"] <- 7.840
```

### Normalize the newly created features using preProcess caret function

```{r}

str(data.combined$ticket.party.size)
head(data.combined$ticket.party.size)
summary(data.combined$ticket.party.size)

str(data.combined$avg.fare)
head(data.combined$avg.fare)
summary(data.combined$avg.fare)


# Normalize numerical variables ticket.party.size and avg.fare
preproc.data.combined <- data.combined[, c("ticket.party.size", "avg.fare")]
preProc <- preProcess(preproc.data.combined, method = c("center", "scale"))

postproc.data.combined <- predict(preProc, preproc.data.combined)


cor(postproc.data.combined$ticket.party.size, postproc.data.combined$avg.fare)
```

0.09 correlation 
Hypothesis refuted for all data. How about for just 1st class all-up?

```{r}
indexes <- which(data.combined$pclass == "1")
cor(postproc.data.combined$ticket.party.size[indexes], 
    postproc.data.combined$avg.fare[indexes])
```

0.26 correlation.
So, hypothesis that ticket.party.size and avg.fare are not correlated for first class.
Hypothesis refuted again.

So, let's see if these two uncorrelated features make any difference 
to the model accuracy.

# Rpart Model with new features

```{r eval=FALSE}
# OK, let's see if our feature engineering has made any difference
features <- c("pclass", "new.title", "family.size", "ticket.party.size", "avg.fare")
rpart.train.3 <- data.combined[1:891, features]

# Run CV and check out results
#rpart.cv <- function(seed, training, labels, ctrl)
rpart.3.cv.1 <- rpart.cv(94622, rpart.train.3, rf.label, ctrl.3)
rpart.3.cv.1

saveRDS(rpart.3.cv.1,  paste0(path,"/RdsObjects/rpart.3.cv.1.rds"))
rm(rpart.3.cv.1)
rpart.3.cv.1
```

```{r}
rpart.3.cv.1 = readRDS(paste0(path,"/RdsObjects/rpart.3.cv.1.rds"))
rpart.3.cv.1

```

## Plot

```{r}
prp(rpart.3.cv.1$finalModel, type = 0, extra = 1, under = TRUE)
```

## Submission

```{r eval=FALSE}
# Subset our test records and features
test.submit.df <- data.combined[892:1309, features]

# Make predictions
rpart.3.preds <- predict(rpart.3.cv.1$finalModel, test.submit.df, type = "class")
table(rpart.3.preds)

# Write out a CSV file for submission to Kaggle
submit.df <- data.frame(PassengerId = rep(892:1309), Survived = rpart.3.preds)

write.csv(submit.df, file = "RPART_SUB_20160619_1.csv", row.names = FALSE)

```

Rpart scores 0.80383

# RandomForest Model with new features

```{r eval=FALSE}
features <- c("pclass", "new.title", "ticket.party.size", "avg.fare")
rf.train.temp <- data.combined[1:891, features]

set.seed(1234)
rf.temp <- randomForest(x = rf.train.temp, y = rf.label, ntree = 1000)
rf.temp

saveRDS(rf.temp,  paste0(path,"/RdsObjects/rf.temp.rds"))
rm(rf.temp)
rf.temp

```

```{r eval=FALSE}
rf.temp = readRDS(paste0(path,"/RdsObjects/rf.temp.rds"))
rf.temp


test.submit.df <- data.combined[892:1309, features]

# Make predictions
rf.preds <- predict(rf.temp, test.submit.df)
table(rf.preds)

# Write out a CSV file for submission to Kaggle
submit.df <- data.frame(PassengerId = rep(892:1309), Survived = rf.preds)

write.csv(submit.df, file = "RF_SUB_20160619_1.csv", row.names = FALSE)

```


Random forest scores 0.80861.

## Submitting, scoring, and some analysis.
Feature engineering using mutual information - 
look at ytube videos on FE or feature selection entropy and dimensional reduction using rtsne library

# See titanic-improving-model.rmd